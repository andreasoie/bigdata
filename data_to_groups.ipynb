{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "#install pytrends\n",
    "#!pip install pytrends\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data based on which year and country we're interested in\n",
    "# Idea: Correct the data by scaling it here and write a new csv-file with this new data, before using it further.\n",
    "\n",
    "def get_historical_dataframe(keyword, timeframe, geo=''):\n",
    "    kw_list = [keyword]\n",
    "    pytrend = TrendReq(hl='en-US', tz=360, timeout=(10,25), retries=2, backoff_factor=0.1, requests_args={'verify':False})\n",
    "    pytrend.build_payload(kw_list, timeframe=timeframe, geo=geo)\n",
    "    \n",
    "    # next we make the dataframe for this word over the period\n",
    "    BP = pytrend.interest_over_time()\n",
    "    if BP.empty:\n",
    "        BP = pd.DataFrame([1,1],columns=[keyword])\n",
    "\n",
    "    return BP\n",
    "\n",
    "def scale_data(df_nor,years,keyword,geo=''):\n",
    "    '''\n",
    "    df_nor: dataframe of one country\n",
    "    years: years we want to stitch together (can get this from df)\n",
    "    geo: code name of country\n",
    "    column: which column we want to scale. 2 for first keyword\n",
    "    returns a dataframe where the specified column is scaled appropriately\n",
    "    '''\n",
    "      \n",
    "    df_nor.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    # get dates where we patch\n",
    "    patch_dates_from = []\n",
    "    patch_dates_to = []\n",
    "    for year in years:\n",
    "        patch_dates_from.append(f\"{year}-06-30\")\n",
    "        patch_dates_to.append(f\"{year}-07-01\")\n",
    "        patch_dates_from.append(f\"{year}-12-31\")\n",
    "        patch_dates_to.append(f\"{year+1}-01-01\")\n",
    "    patch_dates_from.pop()\n",
    "    patch_dates_to.pop()\n",
    "    \n",
    "    for i in range(len(patch_dates_from)):\n",
    "        # prepare data\n",
    "        \n",
    "        i_date_from = df_nor.index[df_nor[\"date\"]==patch_dates_from[i]].tolist()[0]\n",
    "        i_date_to = df_nor.index[df_nor[\"date\"]==patch_dates_to[i]].tolist()[0]\n",
    "        date_from = df_nor[\"date\"][i_date_from]\n",
    "        date_to = df_nor[\"date\"][i_date_to]\n",
    "        timeframe = date_from + \" \" + date_to\n",
    "        #print(timeframe)\n",
    "        P = [df_nor[keyword][i_date_from],df_nor[keyword][i_date_to]]\n",
    "        #print(P)\n",
    "        \n",
    "        BP = get_historical_dataframe(keyword,timeframe,geo)\n",
    "        print(BP)\n",
    "        BP_vals = [BP[keyword][0],BP[keyword][1]]\n",
    "        \n",
    "        # actually do the scaling\n",
    "        s = BP_vals[1]/BP_vals[0]*P[0]/P[1]\n",
    "        #print(df[keyword][0:])\n",
    "        df[keyword][i_date_from:] = df[keyword][i_date_from:]*s\n",
    "        \n",
    "    df[keyword] = 100*df[keyword]/df[keyword].max()\n",
    "    \n",
    "    return df\n",
    "        \n",
    "\n",
    "'''\n",
    "filename = \"bigboy.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "\n",
    "YEARS = [2010]#, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "df_nor = df.loc[df['country'] == \"Norway\"]\n",
    "new_df = scale_data(df_nor,YEARS,\"Facebook\")\n",
    "print(len(df.loc[df['country'] == \"Norway\"]))\n",
    "df.loc[df['country'] == \"Norway\"] = new_df\n",
    "print(new_df)\n",
    "print(len(df.loc[df['country'] == \"Norway\"]))\n",
    "# df.loc[df['country'] == \"Norway\"] = new_df => overwrites only for Norway. \n",
    "# Can loop over each country and search term to make new csv file\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_unavailable_countries(filename: str):\n",
    "    unavailable_countries = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            unavailable_countries.append(line)\n",
    "    return unavailable_countries\n",
    "\n",
    "def load_countries(filename: str, ignore: str):\n",
    "    unavailable_countries = _load_unavailable_countries(ignore)\n",
    "    valid_countries = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line_list = line.split(\",\")\n",
    "            code = line_list[0]\n",
    "            country = line_list[1]\n",
    "            if country not in unavailable_countries:\n",
    "                valid_countries.append((code, country))\n",
    "    return valid_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"bigboy.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df.reset_index(drop=True)\n",
    "to_filename = \"scaled_data.csv\"\n",
    "\n",
    "YEARS = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "search_term = \"Facebook\"\n",
    "country = \"Norway\"\n",
    "\n",
    "country_df = df.loc[df[\"country\"] == country]\n",
    "\n",
    "scale_data(country_df,YEARS,search_term,\"NO\")\n",
    "\n",
    "# before inserting the changed values into the original dataframe, make sure to match indexes\n",
    "country_df.index = df.loc[df['country'] == country].index\n",
    "# insert into dataframe\n",
    "df.loc[df[\"country\"] == country] = country_df\n",
    "\n",
    "search_term = \"Facebook\"\n",
    "country = \"Sweden\"\n",
    "\n",
    "country_df = df.loc[df[\"country\"] == country]\n",
    "\n",
    "scale_data(country_df,YEARS,search_term,\"SE\")\n",
    "\n",
    "# before inserting the changed values into the original dataframe, make sure to match indexes\n",
    "country_df.index = df.loc[df['country'] == country].index\n",
    "# insert into dataframe\n",
    "df.loc[df[\"country\"] == country] = country_df\n",
    "\n",
    "df.to_csv(to_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"bigboy.csv\"\n",
    "to_filename = \"scaled_data.csv\"\n",
    "\n",
    "valid_countries = load_countries(\"docs/countries.txt\",\"docs/ignore.txt\")\n",
    "countries = df[\"country\"].unique()\n",
    "search_terms = df.columns[2:]\n",
    "\n",
    "YEARS = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "df = pd.read_csv(filename)\n",
    "i = 0\n",
    "for search_term in search_terms:\n",
    "    for country in countries:\n",
    "        geo = \"\"\n",
    "        for geocode,check_country in valid_countries:\n",
    "            if country == check_country:\n",
    "                geo = geocode\n",
    "                print(country,geo)\n",
    "        # patch data using search_term,country,geocode\n",
    "        '''\n",
    "        How: \n",
    "        make new df containing only the data for country\n",
    "        scale this data\n",
    "        '''\n",
    "        print(i)\n",
    "        country_df = df.loc[df[\"country\"] == country]\n",
    "\n",
    "        scale_data(country_df,YEARS,search_term,geo)\n",
    "\n",
    "        # before inserting the changed values into the original dataframe, make sure to match indexes\n",
    "        country_df.index = df.loc[df['country'] == country].index\n",
    "        # insert into dataframe\n",
    "        df.loc[df[\"country\"] == country] = country_df\n",
    "    pf.to_csv(f\"scale/try_{i}_{to_filename}\")\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Rate Limit is not publicly known, let me know if you have a consistent estimate\n",
    "* One user reports that 1,400 sequential requests of a 4 hours timeframe got them to the limit. (Replicated on 2 networks)\n",
    "* It has been tested, and 60 seconds of sleep between requests (successful or not) appears to be the correct amount once you reach the limit.\n",
    "time.sleep(60)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(filename,category,glob=False):\n",
    "    '''\n",
    "    filename: Trends/filename (remember .csv). ex: \"db_0.csv\"\n",
    "    category: ex \"Facebook\", \"YouTube\"\n",
    "    returns list of all countries involved (all names of columns/rows) and correlation matrix \n",
    "    '''\n",
    "    df = pd.read_csv(f\"{filename}\")\n",
    "    df[\"date\"]= pd.to_datetime(df[\"date\"])\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Keep country and Facebook\n",
    "    df = df[[\"country\", category]]\n",
    "    \n",
    "    # Get a list of countries\n",
    "    countries = df[\"country\"].unique()\n",
    "    \n",
    "    # Create a new DataFrame \n",
    "    new_df = pd.DataFrame()\n",
    "    for country in countries:\n",
    "        tmp = df[df[\"country\"] == country]#\n",
    "        tmp = tmp.drop(columns=[\"country\"])\n",
    "        tmp = tmp.rename(columns={category: country})\n",
    "        new_df = pd.concat([new_df.reset_index(drop=True), tmp.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    if glob:\n",
    "        # Remove Global, and insert it at the front\n",
    "        new_df[\"Global\"] = new_df.sum(axis=1) / new_df.shape[1]\n",
    "        GLOB = new_df.pop('Global')\n",
    "        new_df.insert(0, \"Global\", GLOB)\n",
    "    \n",
    "    new_df = new_df.sort_index(axis=1)\n",
    "        \n",
    "    CORRELATION_MATRIX = new_df.corr()\n",
    "    \n",
    "    return CORRELATION_MATRIX.columns, CORRELATION_MATRIX\n",
    "    \n",
    "cnt,mtr = get_corr_matrix(\"bigboy.csv\",\"Facebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categories for each file\n",
    "def get_categories(filename):\n",
    "    '''\n",
    "    filename: filename (.csv)\n",
    "    returns categories as a 2D-list where each\n",
    "    inner list refers to a filename\n",
    "    '''    \n",
    "    df = pd.read_csv(f\"{filename}\")\n",
    "    df[\"date\"]= pd.to_datetime(df[\"date\"])\n",
    "    df.dropna(inplace=True)\n",
    "    categories = df.columns[2:]\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corr matrix for bigboy.csv and scaled_data.csv\n",
    "\n",
    "filename = \"bigboy.csv\"\n",
    "category = \"Facebook\"\n",
    "cntr, corr_matrix_1 = get_corr_matrix(filename,category)\n",
    "\n",
    "filename = \"scaled_data.csv\"\n",
    "category = \"Facebook\"\n",
    "cntr, corr_matrix_2 = get_corr_matrix(filename,category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_matrix_1[\"Norway\"][\"Sweden\"],corr_matrix_2[\"Norway\"][\"Sweden\"],corr_matrix_1[\"Norway\"][\"Sweden\"]/corr_matrix_2[\"Norway\"][\"Sweden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corr matrix\n",
    "# prepare to get corr matrices\n",
    "filename = \"bigboy.csv\"\n",
    "categories = get_categories(filename)\n",
    "#print(filename,categories)\n",
    "\n",
    "# get it for each category/term\n",
    "corr_matrices = []\n",
    "for cat in categories:\n",
    "    countries, corr_matrix = get_corr_matrix(filename,cat)\n",
    "    corr_matrices.append([cat,countries,corr_matrix])\n",
    "corr_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_corr_matrix = corr_matrices[0][2].copy()\n",
    "\n",
    "for col in avg_corr_matrix.columns:\n",
    "    avg_corr_matrix[col].values[:] = 0\n",
    "\n",
    "    \n",
    "for i in range(45,50):\n",
    "    i_matrix = corr_matrices[i][2].copy().fillna(1)\n",
    "    avg_corr_matrix += i_matrix\n",
    "\n",
    "\n",
    "avg_corr_matrix /= 5\n",
    "print(avg_corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_matrix(corr_matrix,treshold):\n",
    "    '''\n",
    "    corr_matrix: correlation matrix (dataframe)\n",
    "    treshold: value between 0 and 1\n",
    "    converts matrix to a matrix where all values > treshold gets replaced with 1 and the rest 0\n",
    "    '''\n",
    "    matrix = corr_matrix.copy()\n",
    "    matrix[matrix>=treshold] = 1\n",
    "    matrix[matrix<treshold] = 0\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def make_groups_from_matrix(corr_matrix, treshold, used_countries):\n",
    "    '''\n",
    "    corr_matrix: correlation matrix (dataframe)\n",
    "    treshold: value between 0 and 1\n",
    "    used_countries: countries you don't want to be grouped up. Useful if we already have a group\n",
    "    returns list of lists, where each list is a group\n",
    "    '''\n",
    "    matrix = convert_matrix(corr_matrix,treshold)\n",
    "    #print(corr_matrix,treshold,used_countries)\n",
    "    groups = []\n",
    "    countries = matrix.columns\n",
    "    for c in countries:\n",
    "        if c not in used_countries:\n",
    "            # if country not visited before, make a new group and make sure all correlated countries are added\n",
    "            group = [c]\n",
    "            used_countries.append(c)             \n",
    "            \n",
    "            for ct in group:\n",
    "                # make list of countries that ct correlates with\n",
    "                # append all countries that has not been visited before to group and used_countries\n",
    "                ct_corr = matrix[ct][matrix[ct]==1].index.values\n",
    "                for ctr in ct_corr:\n",
    "                    if ctr not in used_countries:\n",
    "                        used_countries.append(ctr)\n",
    "                        group.append(ctr)\n",
    "            \n",
    "            \n",
    "            groups.append(group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Is for sure making larger groups. Might look nice, but doesn't necessarily say that much\n",
    "# Idea: Make groups with high treshold, then of the remaining countries, use lower treshold, until reach 0 as treshold\n",
    "\n",
    "def groupifier(corr_matrix,start_treshold=0.9,skip_treshold=0.1):\n",
    "    '''\n",
    "    returns groups in a list, and groups on the form: [[treshold,[groups]]xG] (G groups)\n",
    "    '''\n",
    "    groups = []\n",
    "    groups_treshold = []\n",
    "    used_countries = []\n",
    "    steps = int(start_treshold/skip_treshold+1)\n",
    "    for treshold in np.linspace(start_treshold,0,steps):\n",
    "        gr = make_groups_from_matrix(corr_matrix, treshold, used_countries[:])\n",
    "        # copy group\n",
    "        gr_copy = gr[:]\n",
    "        for g in gr_copy:\n",
    "            #print(g)\n",
    "            if len(g)==1:\n",
    "                gr.remove(g)\n",
    "            else:\n",
    "                used_countries+=g\n",
    "                groups.append(g)\n",
    "        if len(gr):\n",
    "            groups_treshold.append([round(treshold,1),gr])\n",
    "\n",
    "    return groups,groups_treshold\n",
    "        \n",
    "comat = corr_matrices[0][2]\n",
    "#print(comat)\n",
    "groups, groups_treshold = groupifier(comat,0.9,0.1)\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Doesn't necessarily give big groups, but makes sure groups are well defined\n",
    "# Meaning: All countries in each group correlates at least treshold to all other countries in that group\n",
    "\n",
    "def groupifier2(corr_matrix,treshold):\n",
    "    '''\n",
    "    corr_matrix: correlation matrix\n",
    "    treshold: number between 0 and 1\n",
    "    returns groups where all countries in each group correlate\n",
    "        at least treshold with each other country in the group\n",
    "    '''\n",
    "    # make a list containing a list with each country\n",
    "    groups = [corr_matrix.columns.values.tolist()]\n",
    "    # idea: split group into sub-groups until each sub-group fullfills criteria\n",
    "    #print(len(groups[0]))\n",
    "    while True:\n",
    "        new_groups = []\n",
    "        for group in groups:\n",
    "            # find countries that correlate least\n",
    "            country1 = \"\"\n",
    "            country2 = \"\"\n",
    "            least_treshold = 1\n",
    "            for c1 in group:\n",
    "                for c2 in group:\n",
    "                    if corr_matrix[c1][c2] < least_treshold:\n",
    "                        country1 = c1\n",
    "                        country2 = c2\n",
    "                        least_treshold = corr_matrix[c1][c2]\n",
    "            if least_treshold < treshold:\n",
    "                break_loop = False\n",
    "                g1 = [country1]\n",
    "                g2 = [country2]\n",
    "                for country in group:\n",
    "                    if country != country1 and country != country2:\n",
    "                        if corr_matrix[country1][country] > corr_matrix[country2][country]:\n",
    "                            g1.append(country)\n",
    "                        else:\n",
    "                            g2.append(country)\n",
    "                new_groups.append(g1)\n",
    "                new_groups.append(g2)\n",
    "            else:\n",
    "                new_groups.append(group)\n",
    "                       \n",
    "        # if no changes are done, break loop. if not, \n",
    "        if new_groups==groups:\n",
    "            break\n",
    "        else:\n",
    "            groups = new_groups\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "comat = corr_matrices[46][2]\n",
    "groups_treshold = groupifier2(comat,0.7)\n",
    "print(groups_treshold)\n",
    "print(len(groups_treshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = make_groups_from_matrix(avg_corr_matrix, 0.8, [])\n",
    "\n",
    "gr2, gr2_tresh = groupifier(avg_corr_matrix)\n",
    "\n",
    "gr3 = groupifier2(avg_corr_matrix,0.8)\n",
    "\n",
    "print(len(gr1),gr1,\"\\n\\n\\n\",len(gr2),gr2_tresh,\"\\n\\n\\n\",len(gr3),gr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gr2_tresh)):\n",
    "    for j in range(len(gr2_tresh[i][1])):\n",
    "        df = pd.DataFrame(gr2_tresh[i][1][j])\n",
    "        df.columns = [gr2_tresh[i][0]]\n",
    "        df.to_csv(f\"groups/treshold_{gr2_tresh[i][0]}_group_{j+1}.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gr1)\n",
    "for i in range(len(gr1)):\n",
    "    df = pd.DataFrame(gr1[i])\n",
    "    df.columns = [f\"group {i+1}\"]\n",
    "    print(df)\n",
    "    df.to_csv(f\"groups2/group_{i+1}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corr_matrices)):\n",
    "    print(corr_matrices[i][0],corr_matrices[i][2][\"Norway\"][\"Sweden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_corr_matrix2 = corr_matrices[0][2].copy()\n",
    "\n",
    "for col in avg_corr_matrix.columns:\n",
    "    avg_corr_matrix2[col].values[:] = 0\n",
    "\n",
    "    \n",
    "for i in range(45,50):\n",
    "    i_matrix = corr_matrices[i][2].copy().fillna(1)\n",
    "    avg_corr_matrix2 += i_matrix\n",
    "\n",
    "\n",
    "avg_corr_matrix2 /= 5\n",
    "print(avg_corr_matrix2,\"\\n\\n\\n\",avg_corr_matrix2[\"Norway\"][\"Sweden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = make_groups_from_matrix(avg_corr_matrix2, 0.5, [])\n",
    "\n",
    "gr2, gr2_tresh = groupifier(avg_corr_matrix2)\n",
    "\n",
    "gr3 = groupifier2(avg_corr_matrix2,0.5)\n",
    "\n",
    "print(len(gr1),gr1,\"\\n\\n\\n\",len(gr2),gr2_tresh,\"\\n\\n\\n\",len(gr3),gr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_corr_matrix = corr_matrices[45][2]\n",
    "\n",
    "G = nx.Graph()\n",
    "for i in range(len(avg_corr_matrix)):\n",
    "    for j in range(i,len(avg_corr_matrix)):\n",
    "        if j != i:\n",
    "            G.add_edge(avg_corr_matrix.columns[i], avg_corr_matrix[avg_corr_matrix.columns[i]].index[j], weight = avg_corr_matrix[avg_corr_matrix.columns[i]][avg_corr_matrix[avg_corr_matrix.columns[i]].index[j]])\n",
    "\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "\n",
    "nc = nx.draw_networkx(G, with_labels=True, node_color='lightgreen', edge_color='#aaaaaa', node_size=350, width=weights, font_size=8, font_color='black', font_weight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
